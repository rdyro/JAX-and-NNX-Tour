{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext matplotlib\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c4484",
   "metadata": {},
   "source": [
    "# What is JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d48b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def add(a, b):\n",
    "  return a + b\n",
    "\n",
    "add(jnp.ones(1), 2 * jnp.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1385853",
   "metadata": {},
   "source": [
    "# How JAX transformations works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(A, x, y):\n",
    "  return jnp.mean((jnp.tanh(x @ A) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = iter(random.split(jax.random.key(17), 1024))\n",
    "x = random.normal(next(keys), (1024,))\n",
    "A = random.normal(next(keys), (1024, 16)) / 1024\n",
    "y = random.normal(next(keys), (16,))\n",
    "print(linear_model(A, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5235731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100 -r 10 linear_model(A, x, y).block_until_ready()\n",
    "%timeit -n 100 -r 10 jax.jit(linear_model)(A, x, y).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc147b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_fn = jax.jit(linear_model)\n",
    "jit_fn(A, x, y)\n",
    "# or simply\n",
    "jax.jit(linear_model)(A, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = jax.grad(linear_model)(A, x, y)\n",
    "dx, dy = jax.grad(linear_model, argnums=(1, 2))(A, x, y)  # provide argnums to get grad wrt non-0 argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.jit(jax.hessian(linear_model, argnums=2))(A, x, y)  # higher order derivatives very fast with JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.make_jaxpr(lambda x: x ** 2)(1))\n",
    "print(\"-------------------------------------\")\n",
    "print(jax.make_jaxpr(jax.grad(lambda x: x ** 2))(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jax.random.normal(next(keys), (128, 1024))\n",
    "Y = jax.random.normal(next(keys), (128, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_axes=(\n",
    "#   None,   # broadcast first argument\n",
    "#   0,      # batch over the first axis\n",
    "#   0       # batch over the first axis\n",
    "# )\n",
    "jax.vmap(linear_model, in_axes=(None, 0, 0))(A, X, Y).shape\n",
    "# 128 losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1fb1b",
   "metadata": {},
   "source": [
    "### Working with pytrees\n",
    "\n",
    "JAX uses pytrees to map over arbitrarily nested containers.\n",
    "\n",
    "* `jax.tree.map` - applies a transformation to data\n",
    "* `jax.tree.leaves` - flattens all data into a list\n",
    "* `jax.tree.structure` - gives you the pytree structure without data\n",
    "* `jax.tree.flatten` - outputs a tuple of struct and flat list of leaves\n",
    "* `jax.tree.unflatten` - combines a pytree structure and a flat list of leaves\n",
    "\n",
    "Most functions accept pytrees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05acd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"layer_embed\": jnp.ones((2, 2)),\n",
    "    \"layers\": [1, 2, 3, jnp.ones(2)],\n",
    "    \"names\": [\"model\", (\"alternative\", \"names\")]\n",
    "}\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.leaves(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327faf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: None, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c9ead",
   "metadata": {},
   "source": [
    "# Neural Networks with NNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8337ad9",
   "metadata": {},
   "source": [
    "| Framework   | Origin      | Key Characteristics             | Status / Notes                                   |\n",
    "|-------------|-------------|---------------------------------|--------------------------------------------------|\n",
    "| haiku       | from GDM    | Early JAX NN library            | **Deprecated**                                   |\n",
    "| equinox     | third party | Simple and powerful stateful/functional library | Has a lot of fans                                |\n",
    "| flax.linen  | from GDM    | Original state-passing Flax interface | In active use, useful for some applications      |\n",
    "| flax.nnx    | from GDM    | New stateful Flax interface     | Supports model surgery, **Recommended for new projects**, Similar to PyTorch |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169da425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, *, rngs: nnx.Rngs):\n",
    "    self.embed = nnx.Linear(1, 128, rngs=rngs)\n",
    "    self.ws = [nnx.Linear(128, 128, rngs=rngs) for _ in range(16)]\n",
    "    self.out_project = nnx.Linear(128, 1, rngs=rngs)\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    z = jax.nn.relu(self.embed(x))\n",
    "    for w in self.ws:\n",
    "      z = jax.nn.relu(w(z))\n",
    "    return self.out_project(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)  # we need a random key generator for deterministic randomness\n",
    "model = Model(rngs=nnx.Rngs(0))  # model initialized like a Python class (it is a Python object)\n",
    "graphdef, state = nnx.split(model)  # split the model into its (graph) definition and its JAX state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can modify model in place, perform surgery\n",
    "print(model.embed.kernel.value)\n",
    "model.embed.kernel.value *= 2\n",
    "print(model.embed.kernel.value)\n",
    "\n",
    "# need to split into state again\n",
    "graphdef, state = nnx.split(model)\n",
    "# or just get the state\n",
    "state = nnx.state(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = jax.tree.reduce(         # reduce to add all byte sizes\n",
    "  lambda x, y: x + y,\n",
    "  jax.tree.map(                       #  map to get byte size of each leaf\n",
    "    lambda x: x.itemsize * x.size,\n",
    "    state)\n",
    ")\n",
    "print(f\"Model size = {model_size / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state is a pytree\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphdef is the model skeleton\n",
    "graphdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit)\n",
    "def forward(state, x):\n",
    "  model = nnx.merge(graphdef, state) # merge the model graph and the model state\n",
    "  return model(x)\n",
    "  \n",
    "@jax.jit\n",
    "def loss_fn(state, x, y):\n",
    "  yp = forward(state, x)\n",
    "  return jnp.mean(jnp.linalg.norm(y - yp, axis=-1) ** 2)\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(loss_fn, argnums=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5769a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data with most points concentrated in the middle\n",
    "\n",
    "X = 3e0 * random.normal(rngs(), (1024, 1))\n",
    "Y = jnp.cos(X) + 1e-1 * random.normal(rngs(), X.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[..., 0], Y[..., 0])\n",
    "Yp = forward(state, X)\n",
    "plt.scatter(X[..., 0], Yp[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-5)\n",
    "\n",
    "@jax.jit # want to compile the top level function\n",
    "def train_step(x, y, state, opt_state):\n",
    "  loss = loss_fn(state, x, y)\n",
    "  gs = jax.grad(loss_fn, argnums=0)(state, x, y) # recomputation, but ok, compiler will merge it\n",
    "  updates, opt_state = optimizer.update(gs, opt_state)\n",
    "  new_state = optax.apply_updates(state, updates)\n",
    "  return loss, new_state, opt_state\n",
    "\n",
    "state = nnx.state(model)\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "pbar = tqdm(range(int(4e3)))\n",
    "for _ in pbar:\n",
    "  ridx = random.randint(rngs(), (64,), 0, X.shape[0])\n",
    "  x, y = X[ridx, :], Y[ridx, :]\n",
    "  l, state, opt_state = train_step(x, y, state, opt_state)\n",
    "  pbar.set_description(f\"loss = {l:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sort = jnp.sort(X, axis=0)\n",
    "Yp_sort = forward(state, X_sort)\n",
    "plt.figure()\n",
    "plt.plot(X_sort[:, 0], Yp_sort[:, 0], color=\"red\")\n",
    "plt.scatter(X[:, 0], Y[:, 0], marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static argnuments are non-numeric or for Python control flow\n",
    "# must be declared explicitly because JAX is trying to compile as much as possible\n",
    "@partial(jax.jit, static_argnames=(\"loss_fn\",)) \n",
    "def train_step(x, y, state, opt_state, loss_fn=None):\n",
    "  loss = loss_fn(state, x, y)\n",
    "  gs = jax.grad(loss_fn, argnums=0)(state, x, y) # recomputation, but ok, compiler will merge it\n",
    "  updates, opt_state = optimizer.update(gs, opt_state)\n",
    "  new_state = optax.apply_updates(state, updates)\n",
    "  return loss, new_state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def weighted_loss_fn(state, x, y):\n",
    "  yp = forward(state, x)\n",
    "  per_example_loss = jnp.sum((y - yp) ** 2, axis=-1)\n",
    "  return jnp.mean(1e1 * jnp.abs(x[..., 0]) * per_example_loss) # weigh examples farther from zero more\n",
    "\n",
    "pbar = tqdm(range(int(4e3)))\n",
    "for _ in pbar:\n",
    "  ridx = random.randint(rngs(), (64,), 0, X.shape[0])\n",
    "  x, y = X[ridx, :], Y[ridx, :]\n",
    "  l, state, opt_state = train_step(x, y, state, opt_state, loss_fn=weighted_loss_fn)\n",
    "  pbar.set_description(f\"loss = {l:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ada55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sort = jnp.sort(X, axis=0)\n",
    "Yp_sort = forward(state, X_sort)\n",
    "plt.figure()\n",
    "plt.plot(X_sort[:, 0], Yp_sort[:, 0], color=\"red\")\n",
    "plt.scatter(X[:, 0], Y[:, 0], marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff61c9",
   "metadata": {},
   "source": [
    "# Robotics + RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling train step from eager mode can have a small overhead\n",
    "\n",
    "# we can unroll the training loop\n",
    "@partial(jax.jit, static_argnames=(\"steps\", \"loss_fn\"))\n",
    "def train_n_steps(x, y, state, opt_state, steps, loss_fn=None):\n",
    "  # steps is a static variable\n",
    "\n",
    "  def loop_body(i, carry):\n",
    "    loss_history, state, opt_state = carry\n",
    "    x_ = jax.lax.dynamic_index_in_dim(x, i, axis=0)  # single batch from a stack of batches\n",
    "    y_ = jax.lax.dynamic_index_in_dim(y, i, axis=0)  # single batch from a stack of batches\n",
    "    loss, state, opt_state = train_step(x_, y_, state, opt_state, loss_fn=loss_fn)\n",
    "    loss_history = jax.lax.dynamic_update_index_in_dim(loss_history, loss, i, axis=0)\n",
    "    return loss_history, state, opt_state\n",
    "    \n",
    "  loss_history = jnp.zeros(steps)\n",
    "  loss_history, state, opt_state = jax.lax.fori_loop(0, steps, loop_body, (loss_history, state, opt_state))\n",
    "  return loss_history, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.state(Model(rngs=rngs))\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "pbar = tqdm(range(int(50)))\n",
    "steps = 100\n",
    "for _ in pbar:\n",
    "  ridx = random.randint(rngs(), (steps * 64,), 0, X.shape[0])\n",
    "  x, y = X[ridx, :], Y[ridx, :]\n",
    "  x, y = x.reshape((steps, 64, 1)), y.reshape((steps, 64, 1))\n",
    "  l, state, opt_state = train_n_steps(x, y, state, opt_state, steps=steps, loss_fn=loss_fn)\n",
    "  pbar.set_description(f\"losses = {l[-1]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22513d63",
   "metadata": {},
   "source": [
    "### What about dynamic iteration count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9568b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for n steps or until you encounter a nan loss\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"steps\", \"loss_fn\"))\n",
    "def train_n_steps_maybe_stop_early(x, y, state, opt_state, steps, loss_fn=None):\n",
    "  # steps is a static variable\n",
    "\n",
    "\n",
    "  def loop_body(i, carry):\n",
    "    loss_history, state, opt_state, done = carry\n",
    "    x_ = jax.lax.dynamic_index_in_dim(x, i, axis=0)\n",
    "    y_ = jax.lax.dynamic_index_in_dim(y, i, axis=0)\n",
    "    loss, state, opt_state = train_step(x_, y_, state, opt_state, loss_fn=loss_fn)\n",
    "    loss_history = jax.lax.dynamic_update_index_in_dim(loss_history, loss, i, axis=0)\n",
    "\n",
    "    done = done | jnp.isnan(loss)  # update the done state\n",
    "\n",
    "    return loss_history, state, opt_state, done\n",
    "\n",
    "  def lazy_loop_body(i, carry):\n",
    "    done = carry[-1]\n",
    "    new_carry = jax.lax.cond(done, \n",
    "                             lambda: carry,\n",
    "                             lambda: loop_body(i, carry))\n",
    "    return new_carry\n",
    "\n",
    "  loss_history = jnp.zeros(steps)\n",
    "  done = False\n",
    "  init_val = (loss_history, state, opt_state, done)\n",
    "  loss_history, state, opt_state, done = jax.lax.fori_loop(0, steps, lazy_loop_body, init_val)\n",
    "  return loss_history, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b23705",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.state(Model(rngs=rngs))\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "pbar = tqdm(range(int(50)))\n",
    "steps = 100\n",
    "for _ in pbar:\n",
    "  ridx = random.randint(rngs(), (steps * 64,), 0, X.shape[0])\n",
    "  x, y = X[ridx, :], Y[ridx, :]\n",
    "  x, y = x.reshape((steps, 64, 1)), y.reshape((steps, 64, 1))\n",
    "  l, state, opt_state = train_n_steps_maybe_stop_early(x, y, state, opt_state, steps=steps, loss_fn=loss_fn)\n",
    "  pbar.set_description(f\"losses = {l[-1]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79d5a5",
   "metadata": {},
   "source": [
    "JAX control flow\n",
    "\n",
    "<img src=\"images/jax_control_flow.png\" style=\"max-width:800px\">\n",
    "<!--![images/jax_control_flow.png](images/jax_control_flow.png)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ad939",
   "metadata": {},
   "source": [
    "# Train on actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader  # use torch dataloader to set up data\n",
    "\n",
    "def collate_fn(data):\n",
    "  return np.stack([arg[0] for arg in data]), np.array([arg[1] for arg in data])\n",
    "\n",
    "ds = datasets.MNIST(\".\", download=True)\n",
    "dl = DataLoader([(np.array(image), cls) for image, cls in tqdm(ds)], 128, shuffle=True, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nnx.Module):\n",
    "  def __init__(self, *, rngs: nnx.Rngs):\n",
    "    channels = 128\n",
    "    self.conv0 = nnx.Conv(1, channels, (3, 3), rngs=rngs)\n",
    "    self.convs = [nnx.Conv(channels, channels, (3, 3), rngs=rngs) for _ in range(32)]\n",
    "    self.output = nnx.Linear(channels * 28 * 28, 1024, rngs=rngs)\n",
    "    self.classifier = nnx.Linear(1024, 10, rngs=rngs)\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    x = x if x.ndim == 4 else x[..., None]\n",
    "    z = jax.nn.relu(self.conv0(x))\n",
    "    for conv in self.convs:\n",
    "      z = z + jax.nn.relu(conv(z))\n",
    "    z = z.reshape((z.shape[0], -1))\n",
    "    self.sow(nnx.Intermediate, \"x_flat\", z)  # if you want to save some values\n",
    "    z = jax.nn.relu(self.output(z))\n",
    "    return self.classifier(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)  # create random keys generator\n",
    "model = MNISTModel(rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c70b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intermediate value by filtering out intermediate state\n",
    "# nnx.split and nnx.state take additional filter arguments\n",
    "# these filter out variable types\n",
    "# - nnx.Param\n",
    "# - nnx.BatchStat\n",
    "# - nnx.Intermediate\n",
    "\n",
    "imgs, classes = next(iter(dl))\n",
    "_ = model(imgs)\n",
    "x_flat = nnx.state(model, nnx.Intermediate).x_flat.value[0]\n",
    "print(x_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "jax.sharding.set_mesh(jax.make_mesh((1,), (\"data\",)))\n",
    "state = jax.device_put(state, P())\n",
    "optimizer = optax.adam(1e-4)\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(imgs, y, state, opt_state):\n",
    "\n",
    "  def loss_fn(state):\n",
    "    model = nnx.merge(graphdef, state)\n",
    "    yp = model(imgs)\n",
    "    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(yp, y))\n",
    "    \n",
    "  with jax.named_scope(\"forward\"):\n",
    "    loss = loss_fn(state)\n",
    "  with jax.named_scope(\"backward\"):\n",
    "    gs = jax.grad(loss_fn)(state)\n",
    "  pred = nnx.merge(graphdef, state)(imgs)\n",
    "  updates, opt_state = optimizer.update(gs, opt_state)\n",
    "  state = optax.apply_updates(state, updates)\n",
    "  return loss, pred, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b809198",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "max_it = int(2e2)\n",
    "pbar = tqdm(enumerate(dl), total=max_it)\n",
    "total, correct = 0, 0\n",
    "train_accuracies = []\n",
    "_ = train_step(imgs, classes, state, opt_state) # precompile train step\n",
    "for i, (imgs, classes) in pbar:\n",
    "  loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)\n",
    "  losses.append(loss)\n",
    "  total, correct = total + imgs.shape[0], correct + jnp.sum(jnp.argmax(pred, -1) == classes)\n",
    "  train_accuracies.append(correct / total)\n",
    "  pbar.set_description(f\"{loss = :.4e}, accuracy {correct / total:%}\")\n",
    "  if i > max_it:\n",
    "    break\n",
    "model = nnx.merge(graphdef, state)  # merge model to insert the optimized state\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracies)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48886376",
   "metadata": {},
   "source": [
    "# Profiling and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71783881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install tensorboard tensorboard-plugin-profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2955c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(state)\n",
    "with jax.profiler.trace(Path(\"~/profiles\").expanduser()):\n",
    "  for _ in range(10):\n",
    "    loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a9b1b",
   "metadata": {},
   "source": [
    "# Unified sharding API (Data / Tensor / Pipeline parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a324eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P, use_mesh, set_mesh\n",
    "\n",
    "A = jnp.ones((16, 16))\n",
    "# a simple mesh with one axis called \"data\" - name is completely arbitrary\n",
    "mesh = jax.make_mesh([jax.device_count()], [\"data\"])\n",
    "set_mesh(mesh)  # set the mesh globally\n",
    "\n",
    "A = jax.device_put(A, P(\"data\", None))\n",
    "jax.debug.visualize_array_sharding(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431223cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jax.device_put(A, P(\"data\", \"data\"))  # cannot shard both axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jax.device_put(jnp.ones((3, 4)), P(\"data\", None))  # cannot shard both axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b866bc",
   "metadata": {},
   "source": [
    "### Data parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb479d2",
   "metadata": {},
   "source": [
    "<!--![images/dp.png](images/dp.png)-->\n",
    "<img src=\"images/dp.png\" style=\"max-width:900px\">\n",
    "\n",
    "Image: Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eaac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, classes = next(iter(dl))\n",
    "\n",
    "imgs = jax.device_put(imgs, P(\"data\", None, None))\n",
    "classes = jax.device_put(classes, P(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89493629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.debug.visualize_array_sharding(imgs)  # visualize_array_sharding works for 1D and 2D\n",
    "imgs.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(imgs[..., 0])\n",
    "jax.debug.visualize_array_sharding(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "state = jax.device_put(state, P())  # model is replicate in DP\n",
    "\n",
    "@jax.jit\n",
    "def train_step(imgs, y, state, opt_state):\n",
    "\n",
    "  def loss_fn(state, x, y):\n",
    "    # with sharding constraints instructs the compiler to shard intermediate value across devices\n",
    "    # this is a hard sharding constraint - the compiler must obey it\n",
    "    # the compiler attempts to make optimal decisions about sharding\n",
    "    x = jax.lax.with_sharding_constraint(x, P(\"data\", None, None))\n",
    "    yp = nnx.merge(graphdef, state)(x)\n",
    "    y, yp = jax.lax.with_sharding_constraint((y, yp), P(\"data\"))\n",
    "    # constraint outputs to ensure data parallelism\n",
    "\n",
    "    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(yp, y))\n",
    "    \n",
    "  with jax.named_scope(\"forward\"):  # for profiling annotations\n",
    "    loss = loss_fn(state, imgs, y)\n",
    "  with jax.named_scope(\"backward\"): # for profiling annotations\n",
    "    gs = jax.grad(loss_fn)(state, imgs, y)\n",
    "  pred = nnx.merge(graphdef, state)(imgs)\n",
    "  updates, opt_state = optimizer.update(gs, opt_state)\n",
    "  state = optax.apply_updates(state, updates)\n",
    "  return loss, pred, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "max_it = int(2e2)\n",
    "pbar = tqdm(enumerate(dl), total=max_it)\n",
    "total, correct = 0, 0\n",
    "train_accuracies = []\n",
    "for i, (imgs, classes) in pbar:\n",
    "  loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)\n",
    "  losses.append(loss)\n",
    "  total, correct = total + imgs.shape[0], correct + jnp.sum(jnp.argmax(pred, -1) == classes)\n",
    "  train_accuracies.append(correct / total)\n",
    "  pbar.set_description(f\"{loss = :.4e}, accuracy {correct / total:%}\")\n",
    "  if i > max_it:\n",
    "    break\n",
    "model = nnx.merge(graphdef, state)  # merge model to insert the optimized state\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracies)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile to compare single device vs DP\n",
    "\n",
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(state)\n",
    "with jax.profiler.trace(Path(\"~/profiles\").expanduser()):\n",
    "  with use_mesh(jax.make_mesh((1,), (\"data\",))):\n",
    "    imgs, classes, state, opt_state  = jax.device_put((imgs, classes, state, opt_state), P())  # place mesh devices\n",
    "    for _ in range(10):\n",
    "      loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)\n",
    "  with use_mesh(jax.make_mesh((4,), (\"data\",))):\n",
    "    imgs, classes, state, opt_state  = jax.device_put((imgs, classes, state, opt_state), P())  # place mesh devices\n",
    "    for _ in range(10):\n",
    "      loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c4857",
   "metadata": {},
   "source": [
    "### FSDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eca756",
   "metadata": {},
   "source": [
    "<!--![images/fsdp.png](images/fsdp.png)-->\n",
    "<img src=\"images/fsdp.png\" style=\"max-width:900px\">\n",
    "\n",
    "Image: Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mesh(jax.make_mesh((jax.device_count(),), (\"fsdp\",)))\n",
    "\n",
    "def shard_weight(x):\n",
    "  if x.ndim == 2 and x.shape[0] % mesh.devices.shape[0] == 0:\n",
    "    return jax.device_put(x, P(\"fsdp\", None))\n",
    "  else:\n",
    "    return jax.device_put(x, P())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(state)\n",
    "\n",
    "# state = jax.device_put(state, P())  # model is replicate in DP\n",
    "state = jax.tree.map(shard_weight, nnx.state(model))  # for FSDP\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"axis\",))\n",
    "def train_step(imgs, y, state, opt_state, axis=None):\n",
    "\n",
    "  def loss_fn(state, x, y):\n",
    "    x = jax.lax.with_sharding_constraint(x, P(axis, None))  # shard data according to `axis` name\n",
    "    yp = nnx.merge(graphdef, state)(x)\n",
    "    y, yp = jax.lax.with_sharding_constraint((y, yp), P(axis))  # shard data according to `axis` name\n",
    "    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(yp, y))\n",
    "    \n",
    "  with jax.named_scope(\"forward\"):  # for profiling annotations\n",
    "    loss = loss_fn(state, imgs, y)\n",
    "  with jax.named_scope(\"backward\"): # for profiling annotations\n",
    "    gs = jax.grad(loss_fn)(state, imgs, y)\n",
    "  pred = nnx.merge(graphdef, state)(imgs)\n",
    "  updates, opt_state = optimizer.update(gs, opt_state)\n",
    "  state = optax.apply_updates(state, updates)\n",
    "  return loss, pred, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df08179",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "max_it = int(2e2)\n",
    "pbar = tqdm(enumerate(dl), total=max_it)\n",
    "total, correct = 0, 0\n",
    "train_accuracies = []\n",
    "for i, (imgs, classes) in pbar:\n",
    "  loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state)\n",
    "  losses.append(loss)\n",
    "  total, correct = total + imgs.shape[0], correct + jnp.sum(jnp.argmax(pred, -1) == classes)\n",
    "  train_accuracies.append(correct / total)\n",
    "  pbar.set_description(f\"{loss = :.4e}, accuracy {correct / total:%}\")\n",
    "  if i > max_it:\n",
    "    break\n",
    "model = nnx.merge(graphdef, state)  # merge model to insert the optimized state\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93621b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile to compare single device vs DP vs FSDP\n",
    "model = MNISTModel(rngs=rngs)\n",
    "graphdef, state = nnx.split(model)\n",
    "optimizer = optax.adam(1e-3)\n",
    "\n",
    "with jax.profiler.trace(Path(\"~/profiles\").expanduser()):\n",
    "  with use_mesh(jax.make_mesh((1,), (\"data\",))):\n",
    "    imgs, classes, state = jax.device_put((imgs, classes, state), P())  # place mesh devices\n",
    "    opt_state = optimizer.init(state)\n",
    "    for _ in range(10):\n",
    "      loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state, axis=None)\n",
    "  with use_mesh(jax.make_mesh((4,), (\"data\",))):\n",
    "    imgs, classes, state = jax.device_put((imgs, classes, state), P())  # place mesh devices\n",
    "    opt_state = optimizer.init(state)\n",
    "    for _ in range(10):\n",
    "      loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state, axis=\"data\")\n",
    "  with use_mesh(jax.make_mesh((4,), (\"fsdp\",))):\n",
    "    imgs, classes, state = jax.device_put((imgs, classes, state), P())  # place mesh devices\n",
    "    state = jax.tree.map(shard_weight, state)\n",
    "    opt_state = optimizer.init(state)\n",
    "    for _ in range(10):\n",
    "      loss, pred, state, opt_state = train_step(imgs, classes, state, opt_state, axis=\"fsdp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57774972",
   "metadata": {},
   "source": [
    "# More useful JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1951720",
   "metadata": {},
   "source": [
    "### Use multiple CPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c16820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_num_cpu_devices\", 8)\n",
    "print(jax.devices(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e4d98",
   "metadata": {},
   "source": [
    "### Eval shape - abstract function evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf121dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enormous_array():\n",
    "  return random.normal(random.key(0), (1024, 1024, 1024, 1024)) # 1 TB\n",
    "  \n",
    "A = jax.eval_shape(enormous_array)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45aab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nnx.eval_shape(lambda: MNISTModel(rngs=nnx.Rngs(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada079f",
   "metadata": {},
   "source": [
    "### Ahead of time compilation\n",
    "### [https://openxla.org/](https://openxla.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def square(x):\n",
    "  return x ** 2\n",
    "  \n",
    "fn = square.lower(jnp.ones(10)).compile()\n",
    "print(fn.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with abstract values, but XLA will warn you about trying to allocate Petabytes of data\n",
    "fn = square.lower(jax.ShapeDtypeStruct((2 ** 50,), jnp.float32)).compile()\n",
    "print(fn.as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_compiled = train_step.lower(imgs, classes, state, opt_state, axis=None).compile()\n",
    "print(f\"Train step is {len(train_step_compiled.as_text().split('\\n'))} lines of HLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1be9e5",
   "metadata": {},
   "source": [
    "<img src=\"images/escape_hatches.png\" style=\"max-width:900px\">\n",
    "<!--![images/escape_hatches.png](images/escape_hatches.png)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b96b20",
   "metadata": {},
   "source": [
    "### Explicit Communication: [shard_map](https://docs.jax.dev/en/latest/notebooks/shard_map.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax.experimental.shard_map import shard_map\n",
    "\n",
    "\n",
    "@partial(shard_map, mesh=mesh, \n",
    "    in_specs=(P('x', 'y'), P('y', None)),\n",
    "    out_specs=P('x', None))\n",
    "def matmul_basic(a_block, b_block):\n",
    "  # a_block: f32[2, 8]\n",
    "  # b_block: f32[8, 4]\n",
    "\n",
    "  # compute\n",
    "  z_partialsum = jnp.dot(a_block, b_block)\n",
    "\n",
    "  # communicate\n",
    "  z_block = jax.lax.psum(z_partialsum, 'y')\n",
    "\n",
    "  # c_block: f32[2, 4]\n",
    "  return z_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a57d",
   "metadata": {},
   "source": [
    "### Kernel languages: Pallas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1802b",
   "metadata": {},
   "source": [
    "<img src=\"images/pallas.png\" style=\"max-width:900px\">\n",
    "<!--![images/pallas.png](images/pallas.png)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c00be",
   "metadata": {},
   "source": [
    "### Advanced checkpointing with Orbax\n",
    "### [https://github.com/google/orbax](https://github.com/google/orbax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
